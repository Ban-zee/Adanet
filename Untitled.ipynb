{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banzee/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dill\n",
    "\n",
    "import numpy as np\n",
    "import data_pre_processing as dp\n",
    "\n",
    "\n",
    "from keras import backend as k\n",
    "from keras import optimizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Input, Dense, concatenate, add\n",
    "from keras.models import Model, load_model\n",
    "from keras.regularizers import l1\n",
    "from itertools import chain\n",
    "from sklearn.metrics import accuracy_score\n",
    "from shutil import copyfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopEarly(Callback):\n",
    "    def __init__(self, threshold, metric=\"val_acc\", verbose=True):\n",
    "        super(StopEarly, self).__init__()\n",
    "        self.threshold = threshold\n",
    "        self.metric = metric\n",
    "        self.last_value = 0\n",
    "        self.stopped_epoch = 0\n",
    "        self.notChanged = 0\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.metric)\n",
    "        if logs.get(self.metric) - self.last_value < self.threshold:\n",
    "            if self.notChanged >= 3:\n",
    "                self.model.stop_training = True\n",
    "                self.stopped_epoch = epoch\n",
    "            else:\n",
    "                self.notChanged += 1\n",
    "        else:\n",
    "            self.notChanged = 0\n",
    "        self.last_value = current\n",
    "\n",
    "    def on_train_end(self, log={}):\n",
    "        if self.stopped_epoch > 0 and self.verbose:\n",
    "            print(\n",
    "                \"model stopped training on epoch\",\n",
    "                self.stopped_epoch,\n",
    "                \"with val_acc =\",\n",
    "                self.last_value,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runthrough(T, depth, layerDic):\n",
    "    for i in range(depth):\n",
    "        for t in range(T):\n",
    "            for prefix in (\"c\", \"\"):\n",
    "                name = prefix + str(i) + \".\" + str(t)\n",
    "                try:\n",
    "                    print(name, layerDic[name])\n",
    "                except:\n",
    "                    pass\n",
    "    for name in (\"c.out\", \"output.Layer\"):\n",
    "        try:\n",
    "            print(name, layerDic[name])\n",
    "        except:\n",
    "            pass\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toSymbolicDict(T, depth, layerDic):\n",
    "    tensorDic = {}\n",
    "    key = \"feeding.Layer\"\n",
    "    params = layerDic[key]\n",
    "    tensorDic[key] = Input(shape=params[1][\"shape\"], name=key)\n",
    "\n",
    "    for i in range(depth):\n",
    "        for t in range(T):\n",
    "            for prefix in (\"c\", \"\"):\n",
    "                key = prefix + str(i) + \".\" + str(t)\n",
    "                try:\n",
    "                    params = layerDic[key]\n",
    "                    if key[0] == \"c\":  # cocatenating layer\n",
    "                        candidateLayers = Call(tensorDic, params[1])\n",
    "                        tensorDic[key] = params[0](candidateLayers)\n",
    "                    elif key != \"output.Layer\":\n",
    "                        tensorDic[key] = params[0](\n",
    "                            params[1][\"units\"],\n",
    "                            activation=params[1][\"activation\"],\n",
    "                            name=key,\n",
    "                        )(tensorDic[params[2]])\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    key = \"c.out\"\n",
    "    try:\n",
    "        params = layerDic[key]\n",
    "        candidateLayers = Call(tensorDic, params[1])\n",
    "        tensorDic[key] = params[0](candidateLayers)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    key = \"output.Layer\"\n",
    "    params = layerDic[key]\n",
    "    tensorDic[key] = params[0](\n",
    "        params[1][\"units\"], activation=params[1][\"activation\"], name=key\n",
    "    )(tensorDic[params[2]])\n",
    "\n",
    "    return tensorDic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_new(\n",
    "    B,\n",
    "    T,\n",
    "    flat_image,\n",
    "    lr,\n",
    "    reps,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    epochs,\n",
    "    size,\n",
    "    epsilon,\n",
    "    pathToSaveModel,\n",
    "    probaThreshold,\n",
    "    handleMultipleInput,\n",
    "    lambda1,\n",
    "):\n",
    "    count = 1\n",
    "    layerDic = {}\n",
    "    layersNamesToOutput = []\n",
    "    concatOutName = \"c.out\"\n",
    "\n",
    "    earlyStopping = StopEarly(0.0001, \"val_acc\", True)\n",
    "\n",
    "    layerDic[\"feeding.Layer\"] = (\n",
    "        Input,\n",
    "        {\"shape\": (flat_image,), \"name\": \"feeding.Layer\"},\n",
    "    )\n",
    "\n",
    "    for t in range(T):\n",
    "        changed = False  # boolean to track if the base model has improved (improved)\n",
    "        print(\"\\n\\n\" + 100 * \"=\" + \"\\niteration n.\" + str(t) + \"\\n\" + 100 * \"=\")\n",
    "        if t == 0:\n",
    "            layerName = \"0.0\"\n",
    "            layerDic[layerName] = (\n",
    "                Dense,\n",
    "                {\n",
    "                    \"units\": B,\n",
    "                    \"activation\": \"relu\",\n",
    "                    \"kernel_regularizer\": l1(lambda1),\n",
    "                    \"name\": layerName,\n",
    "                },\n",
    "                \"feeding.Layer\",\n",
    "            )\n",
    "            layerDic[\"output.Layer\"] = (\n",
    "                Dense,\n",
    "                {\n",
    "                    \"units\": 1,\n",
    "                    \"activation\": \"sigmoid\",\n",
    "                    \"kernel_regularizer\": l1(lambda1),\n",
    "                    \"name\": \"output.Layer\",\n",
    "                },\n",
    "                layerName,\n",
    "            )\n",
    "            layersNamesToOutput.append(layerName)\n",
    "            previousScore = float(\"Inf\")\n",
    "\n",
    "            symbolicTensorsDict = toSymbolicDict(1, 1, layerDic)\n",
    "            model = Model(\n",
    "                inputs=symbolicTensorsDict[\"feeding.Layer\"],\n",
    "                outputs=symbolicTensorsDict[\"output.Layer\"],\n",
    "            )\n",
    "            model.compile(\n",
    "                optimizer=optimizers.SGD(lr=lr, decay=1e-6, momentum=0.5),\n",
    "                loss=\"binary_crossentropy\",\n",
    "                metrics=[\"accuracy\"],\n",
    "            )\n",
    "            model.fit(\n",
    "                x=x_train,\n",
    "                y=y_train,\n",
    "                validation_split=0.1,\n",
    "                callbacks=[earlyStopping],\n",
    "                epochs=epochs,\n",
    "                batch_size=size,\n",
    "                verbose=0,\n",
    "            )\n",
    "            model.save_weights(\"w_\" + pathToSaveModel)\n",
    "            model.save(pathToSaveModel)\n",
    "\n",
    "            with open(\"layerDic.pkl\", \"wb\") as dicFile:\n",
    "                dill.dump(layerDic, dicFile)\n",
    "            with open(\"layersNamesToOutput.pkl\", \"wb\") as outFile:\n",
    "                dill.dump(layersNamesToOutput, outFile)\n",
    "            k.clear_session()\n",
    "        else:\n",
    "\n",
    "            if t > 1:\n",
    "                copyfile(pathToSaveModel, str(t - 1) + pathToSaveModel)\n",
    "                copyfile(\"w_\" + pathToSaveModel, \"w_\" + str(t - 1) + pathToSaveModel)\n",
    "                try:\n",
    "                    os.rename(\"best_\" + pathToSaveModel, pathToSaveModel)\n",
    "                    os.rename(\"best_w_\" + pathToSaveModel, \"w_\" + pathToSaveModel)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            for rep in range(reps):\n",
    "                print(\"\\n rep \" + str(rep))\n",
    "\n",
    "                modelTest = load_model(pathToSaveModel)\n",
    "\n",
    "                previousDepth = getPreviousDepth(layerDic, t)\n",
    "                previousPredictions = classPrediction(modelTest, x_test, probaThreshold)\n",
    "\n",
    "                with open(\"layerDic.pkl\", \"rb\") as f:\n",
    "                    layerDic = dill.load(f)\n",
    "                with open(\"layersNamesToOutput.pkl\", \"rb\") as f:\n",
    "                    layersNamesToOutput = dill.load(f)\n",
    "                currentDepth = previousDepth + 1\n",
    "\n",
    "                for depth in range(currentDepth):\n",
    "                    layerName = str(depth) + \".\" + str(t)\n",
    "\n",
    "                    concatLayerName = \"c\" + layerName\n",
    "                    if handleMultipleInput == \"concatenate\":\n",
    "                        functionChoice = concatenate\n",
    "                    elif handleMultipleInput == \"add\":\n",
    "                        functionChoice = add\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            \"handleMultipleInput must have a value in ('concatenate','add')\"\n",
    "                        )\n",
    "\n",
    "                    if depth == 0:\n",
    "                        layerDic[layerName] = (\n",
    "                            Dense,\n",
    "                            {\"units\": B, \"activation\": \"relu\", \"name\": layerName},\n",
    "                            \"feeding.Layer\",\n",
    "                        )\n",
    "                    else:\n",
    "                        candidateNameList = selectCandidateLayers(layerDic, t, depth)\n",
    "                        candidateNameList = drawing(candidateNameList)\n",
    "                        layerBelowName = str(depth - 1) + \".\" + str(t)\n",
    "                        candidateNameList.append(layerBelowName)\n",
    "                        candidateNameList = list(set(candidateNameList))\n",
    "                        if len(candidateNameList) > 1:\n",
    "                            layerDic[concatLayerName] = (\n",
    "                                functionChoice,\n",
    "                                candidateNameList,\n",
    "                            )\n",
    "                            layerDic[layerName] = (\n",
    "                                Dense,\n",
    "                                {\n",
    "                                    \"units\": B,\n",
    "                                    \"activation\": \"relu\",\n",
    "                                    \"kernel_regularizer\": l1(lambda1),\n",
    "                                    \"name\": layerName,\n",
    "                                },\n",
    "                                concatLayerName,\n",
    "                            )\n",
    "                        else:\n",
    "                            layerDic[layerName] = (\n",
    "                                Dense,\n",
    "                                {\n",
    "                                    \"units\": B,\n",
    "                                    \"activation\": \"relu\",\n",
    "                                    \"kernel_regularizer\": l1(lambda1),\n",
    "                                    \"name\": layerName,\n",
    "                                },\n",
    "                                candidateNameList[0],\n",
    "                            )\n",
    "                    if depth == currentDepth - 1:\n",
    "                        layersNamesToOutput.append(layerName)\n",
    "\n",
    "                if len(layersNamesToOutput) > 1:\n",
    "                    layerDic[concatOutName] = (\n",
    "                        functionChoice,\n",
    "                        list(set(layersNamesToOutput)),\n",
    "                    )\n",
    "                    layerDic[\"output.Layer\"] = (\n",
    "                        Dense,\n",
    "                        {\n",
    "                            \"units\": 1,\n",
    "                            \"activation\": \"sigmoid\",\n",
    "                            \"kernel_regularizer\": l1(lambda1),\n",
    "                            \"name\": \"output.Layer\",\n",
    "                        },\n",
    "                        concatOutName,\n",
    "                    )\n",
    "                else:\n",
    "                    layerDic[\"output.Layer\"] = (\n",
    "                        Dense,\n",
    "                        {\n",
    "                            \"units\": 1,\n",
    "                            \"activation\": \"sigmoid\",\n",
    "                            \"kernel_regularizer\": l1(lambda1),\n",
    "                            \"name\": \"output.Layer\",\n",
    "                        },\n",
    "                        layersNamesToOutput[0],\n",
    "                    )\n",
    "\n",
    "                symbolicTensorsDict = toSymbolicDict(t + 1, currentDepth + 1, layerDic)\n",
    "\n",
    "                model = Model(\n",
    "                    inputs=symbolicTensorsDict[\"feeding.Layer\"],\n",
    "                    outputs=symbolicTensorsDict[\"output.Layer\"],\n",
    "                )\n",
    "                model.compile(\n",
    "                    optimizer=optimizers.SGD(\n",
    "                        lr=lr, momentum=0.8\n",
    "                    ),\n",
    "                    loss=\"binary_crossentropy\",\n",
    "                    metrics=[\"accuracy\"],\n",
    "                )\n",
    "\n",
    "                model.load_weights(\"w_\" + pathToSaveModel, by_name=True)\n",
    "\n",
    "                model.fit(\n",
    "                    x=x_train,\n",
    "                    y=y_train,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[earlyStopping],\n",
    "                    epochs=epochs,\n",
    "                    batch_size=size,\n",
    "                    verbose=1,\n",
    "                )\n",
    "                print(\"fitted model number \", count)\n",
    "                count += 1\n",
    "                currentPredictions = classPrediction(model, x_test, probaThreshold)\n",
    "                currentScore = objectiveFunction(\n",
    "                    y_test, previousPredictions, currentPredictions\n",
    "                )\n",
    "\n",
    "                if previousScore - currentScore > epsilon:\n",
    "                    print(\"saving better model\")\n",
    "                    changed = True\n",
    "\n",
    "                    previousScore = currentScore\n",
    "                    model.save(\"best_\" + pathToSaveModel)\n",
    "                    model.save_weights(\"best_w_\" + pathToSaveModel)\n",
    "                    with open(\"layersNamesToOutput.pkl\", \"wb\") as f:\n",
    "                        dill.dump(layersNamesToOutput, f)\n",
    "                    with open(\"layerDic.pkl\", \"wb\") as f:\n",
    "                        dill.dump(layerDic, f)\n",
    "                k.clear_session()\n",
    "            if not changed:\n",
    "                print(\"model not improved at iteration\", t, \"stopping early\")\n",
    "                return\n",
    "    bestModel = load_model(\"best_\" + pathToSaveModel)\n",
    "    print(bestModel.metric_names)\n",
    "    print(\"Test metrics : \", bestModel.evaluate(x_test, y_test))\n",
    "    k.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawing(candidates):\n",
    "    result = np.random.choice(candidates, size=np.random.randint(0, len(candidates)), replace=False)\n",
    "    return result.tolist()\n",
    "\n",
    "\n",
    "def getPreviousDepth(layerDic, t):\n",
    "    previousDepth = 0\n",
    "    for layerName in layerDic.keys():\n",
    "        depth, iteration = layerName.split(\".\")\n",
    "        try:\n",
    "            depth_int, iteration_int = int(depth), int(iteration)\n",
    "            if iteration_int == t - 1 and depth_int > previousDepth:\n",
    "                previousDepth = depth_int\n",
    "        except:\n",
    "            pass\n",
    "    return previousDepth + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawing(candidates):\n",
    "    result = np.random.choice(candidates, size=np.random.randint(0, len(candidates)), replace=False)\n",
    "    return result.tolist()\n",
    "\n",
    "\n",
    "def getPreviousDepth(layerDic, t):\n",
    "    previousDepth = 0\n",
    "    for layerName in layerDic.keys():\n",
    "        depth, iteration = layerName.split(\".\")\n",
    "        try:\n",
    "            depth_int, iteration_int = int(depth), int(iteration)\n",
    "            if iteration_int == t - 1 and depth_int > previousDepth:\n",
    "                previousDepth = depth_int\n",
    "        except:\n",
    "            pass\n",
    "    return previousDepth + 1\n",
    "\n",
    "\n",
    "def selectCandidateLayers(layerDic, t, c):\n",
    "\n",
    "    candidateList = []\n",
    "    for layerName in layerDic.keys():\n",
    "        depth, iteration = layerName.split(\".\")\n",
    "        try:\n",
    "            depth_int, iteration_int = int(depth), int(iteration)\n",
    "            if depth_int == c - 1:\n",
    "                candidateList.append(layerName)\n",
    "        except:\n",
    "            pass\n",
    "    return candidateList\n",
    "\n",
    "\n",
    "def Call(dic, keys):\n",
    "    return [dic[key] for key in keys]\n",
    "\n",
    "\n",
    "def classPrediction(model, x, probaThreshold):\n",
    "    probas = np.array(model.predict(x))\n",
    "    booleans = probas >= probaThreshold\n",
    "    booleans = list(chain(*booleans))\n",
    "    classes = []\n",
    "    for boolean in booleans:\n",
    "        if boolean:\n",
    "            classes.append(1)\n",
    "        else:\n",
    "            classes.append(-1)\n",
    "    return classes\n",
    "\n",
    "\n",
    "def objectiveFunction(trueLabels, previousPredictions, currentPredictions):\n",
    "\n",
    "    result = 0\n",
    "    for i in range(len(trueLabels)):\n",
    "        result += np.exp(\n",
    "            1\n",
    "            - trueLabels[i] * previousPredictions[i]\n",
    "            - trueLabels[i] * currentPredictions[i]\n",
    "        )\n",
    "    result = result / len(trueLabels)\n",
    "    return result\n",
    "\n",
    "\n",
    "def Encoding(y_vect):\n",
    "    return np.array([0 if i == y_vect[0] else 1 for i in y_vect])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    pathToSaveModel = \"bestModel.h5\"\n",
    "    imsize = 32\n",
    "    flattenDimIm = imsize * imsize * 3\n",
    "    B = 150\n",
    "    T = 10\n",
    "    lr = 10e-3\n",
    "    reps = 5\n",
    "    trainNum = 5000\n",
    "    testNum = 1000\n",
    "    epochs = 1000\n",
    "    batchSize = 32\n",
    "    epsilon = 0.0001\n",
    "    labels = [3, 5]\n",
    "    probaThreshold = 0.5\n",
    "    handleMultipleInput = \"add\"\n",
    "    lambda1 = 0.000001\n",
    "\n",
    "    print(\"B\", B)\n",
    "    print(\"T\", T)\n",
    "    print(\"lr\", lr)\n",
    "    print(\"epsilon\", epsilon)\n",
    "    print(\"labels\", labels)\n",
    "    print(\"lambda1\", lambda1)\n",
    "\n",
    "    if len(labels) > 2 or labels[0] == labels[1]:\n",
    "        raise ValueError(\"labels must be array of 2 distinct values\")\n",
    "    for i in range(2):\n",
    "        if labels[i] < 0 or labels[i] > 9:\n",
    "            raise ValueError(\"label value must be between 0 and 9 included\")\n",
    "\n",
    "    train, test = dp.loadRawData()\n",
    "    x_train, y_train = dp.loadTrainingData(train, labels, trainNum)\n",
    "    x_test, y_test = dp.loadTestingData(test, labels, testNum)\n",
    "\n",
    "    x_train = x_train.flatten().reshape(trainNum, flattenDimIm) / 255\n",
    "    x_test = x_test.flatten().reshape(testNum, flattenDimIm) / 255\n",
    "\n",
    "    y_train = Encoding(y_train)\n",
    "    y_test = Encoding(y_test)\n",
    "\n",
    "    build_new(\n",
    "        B,\n",
    "        T,\n",
    "        flattenDimIm,\n",
    "        lr,\n",
    "        reps,\n",
    "        x_train,\n",
    "        y_train[:trainNum],\n",
    "        x_test,\n",
    "        y_test,\n",
    "        epochs,\n",
    "        batchSize,\n",
    "        epsilon,\n",
    "        pathToSaveModel,\n",
    "        probaThreshold,\n",
    "        handleMultipleInput,\n",
    "        lambda1,\n",
    "    )\n",
    "\n",
    "    model = load_model(pathToSaveModel)\n",
    "\n",
    "    preds = np.round(model.predict(x_test))\n",
    "    print(accuracy_score(preds, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B 150\n",
      "T 10\n",
      "lr 0.01\n",
      "epsilon 0.0001\n",
      "labels [3, 5]\n",
      "lambda1 1e-06\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "iteration n.0\n",
      "====================================================================================================\n",
      "WARNING:tensorflow:From /home/banzee/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/banzee/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "model stopped training on epoch 198 with val_acc = 0.6100000009536743\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "iteration n.1\n",
      "====================================================================================================\n",
      "\n",
      " rep 0\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1000\n",
      "4500/4500 [==============================] - 4s 976us/step - loss: 8.0184 - acc: 0.4953 - val_loss: 8.0031 - val_acc: 0.4980\n",
      "fitted model number  1\n",
      "saving better model\n",
      "\n",
      " rep 1\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1000\n",
      "4500/4500 [==============================] - 4s 983us/step - loss: 7.5745 - acc: 0.5271 - val_loss: 8.0268 - val_acc: 0.5020\n",
      "Epoch 2/1000\n",
      "4500/4500 [==============================] - 4s 900us/step - loss: 7.8549 - acc: 0.5127 - val_loss: 8.0268 - val_acc: 0.5020\n",
      "Epoch 3/1000\n",
      "4500/4500 [==============================] - 4s 989us/step - loss: 7.8549 - acc: 0.5127 - val_loss: 8.0268 - val_acc: 0.5020\n",
      "Epoch 4/1000\n",
      "4500/4500 [==============================] - 4s 982us/step - loss: 7.8549 - acc: 0.5127 - val_loss: 8.0268 - val_acc: 0.5020\n",
      "Epoch 5/1000\n",
      "4500/4500 [==============================] - 5s 1ms/step - loss: 7.8549 - acc: 0.5127 - val_loss: 8.0268 - val_acc: 0.5020\n",
      "model stopped training on epoch 4 with val_acc = 0.502\n",
      "fitted model number  2\n",
      "\n",
      " rep 2\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1000\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 7.9361 - acc: 0.4964 - val_loss: 8.0031 - val_acc: 0.4980\n",
      "fitted model number  3\n",
      "\n",
      " rep 3\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1000\n",
      "4500/4500 [==============================] - 5s 1ms/step - loss: 7.4098 - acc: 0.5260 - val_loss: 8.0031 - val_acc: 0.4980\n",
      "fitted model number  4\n",
      "\n",
      " rep 4\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1000\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 7.5471 - acc: 0.5280 - val_loss: 8.0268 - val_acc: 0.5020\n",
      "Epoch 2/1000\n",
      "4500/4500 [==============================] - 4s 965us/step - loss: 7.8549 - acc: 0.5127 - val_loss: 8.0268 - val_acc: 0.5020\n",
      "Epoch 3/1000\n",
      "4500/4500 [==============================] - 4s 977us/step - loss: 7.8549 - acc: 0.5127 - val_loss: 8.0268 - val_acc: 0.5020\n",
      "Epoch 4/1000\n",
      "4500/4500 [==============================] - 4s 964us/step - loss: 7.8549 - acc: 0.5127 - val_loss: 8.0268 - val_acc: 0.5020\n",
      "Epoch 5/1000\n",
      "4500/4500 [==============================] - 4s 968us/step - loss: 7.8549 - acc: 0.5127 - val_loss: 8.0268 - val_acc: 0.5020\n",
      "model stopped training on epoch 4 with val_acc = 0.502\n",
      "fitted model number  5\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "iteration n.2\n",
      "====================================================================================================\n",
      "\n",
      " rep 0\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1000\n",
      "4500/4500 [==============================] - 8s 2ms/step - loss: 8.1731 - acc: 0.4873 - val_loss: 8.0031 - val_acc: 0.4980\n",
      "fitted model number  6\n",
      "saving better model\n",
      "\n",
      " rep 1\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1000\n",
      "4500/4500 [==============================] - 7s 2ms/step - loss: 8.1731 - acc: 0.4873 - val_loss: 8.0031 - val_acc: 0.4980\n",
      "fitted model number  7\n",
      "\n",
      " rep 2\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1000\n",
      "4500/4500 [==============================] - 7s 2ms/step - loss: 8.1731 - acc: 0.4873 - val_loss: 8.0031 - val_acc: 0.4980\n",
      "fitted model number  8\n",
      "\n",
      " rep 3\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1000\n",
      "4500/4500 [==============================] - 8s 2ms/step - loss: 8.1731 - acc: 0.4873 - val_loss: 8.0031 - val_acc: 0.4980\n",
      "fitted model number  9\n",
      "\n",
      " rep 4\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1000\n",
      "4500/4500 [==============================] - 7s 2ms/step - loss: 8.1731 - acc: 0.4873 - val_loss: 8.0031 - val_acc: 0.4980\n",
      "fitted model number  10\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "iteration n.3\n",
      "====================================================================================================\n",
      "\n",
      " rep 0\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1000\n",
      "4500/4500 [==============================] - 11s 2ms/step - loss: 8.1731 - acc: 0.4873 - val_loss: 8.0031 - val_acc: 0.4980\n",
      "fitted model number  11\n",
      "\n",
      " rep 1\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1000\n",
      "4500/4500 [==============================] - 10s 2ms/step - loss: 8.1731 - acc: 0.4873 - val_loss: 8.0031 - val_acc: 0.4980\n",
      "fitted model number  12\n",
      "\n",
      " rep 2\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1000\n",
      "4500/4500 [==============================] - 9s 2ms/step - loss: 8.1731 - acc: 0.4873 - val_loss: 8.0031 - val_acc: 0.4980\n",
      "fitted model number  13\n",
      "\n",
      " rep 3\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1000\n",
      "4500/4500 [==============================] - 10s 2ms/step - loss: 8.1731 - acc: 0.4873 - val_loss: 8.0031 - val_acc: 0.4980\n",
      "fitted model number  14\n",
      "\n",
      " rep 4\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/1000\n",
      "4500/4500 [==============================] - 10s 2ms/step - loss: 8.1731 - acc: 0.4873 - val_loss: 8.0031 - val_acc: 0.4980\n",
      "fitted model number  15\n",
      "model not improved at iteration 3 stopping early\n",
      "0.497\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
